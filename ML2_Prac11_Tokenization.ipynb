{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVN74nbV5kpK"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model\n",
        "try:\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "   print(\"Downloading 'en_core_web_sm' model. Please wait...\")\n",
        "   spacy.cli.download(\"en_core_web_sm\")\n",
        "   nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mpTMtSsU6Jjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "9m6yUCTv6Y8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INPUT TEXT ---\n",
        "text = \"Jack and Jill went up the hill\"\n",
        "print(f\"--- Original Text ---\\n'{text}'\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sL-6hAy6bNt",
        "outputId": "22c46d01-c7b4-4695-9d1e-c989f11a8f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Original Text ---\n",
            "'Jack and Jill went up the hill'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the text with spaCy\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "8_GAkJP56jDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store results\n",
        "tokens = []\n",
        "clean_tokens = []\n",
        "lemmas = []\n",
        "stems = []"
      ],
      "metadata": {
        "id": "eS7cs9jW6j_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Tokenization, Lemmatization, Stop Word Removal, and Stemming ---\n",
        "print(\"--- Step-by-Step Processing ---\")\n",
        "for token in doc:\n",
        "# 1. Tokenization (The token object itself is a token)\n",
        " tokens.append(token.text)\n",
        "\n",
        "# 3. Lemmatization (using token.lemma_)\n",
        " lemma = token.lemma_\n",
        " lemmas.append(lemma)\n",
        "\n",
        "# 4. Stop Word Removal (check if the token is a stop word and if it's punctuation/whitespace)\n",
        "# Lowercasing is good practice for stop word check\n",
        " is_stop = token.is_stop or token.is_punct or token.is_space\n",
        " if not is_stop:\n",
        "# Append the lemmatized form after stop-word removal for 'clean' tokens\n",
        "  clean_tokens.append(lemma)\n",
        "\n",
        "# 2. Stemming (using NLTK's PorterStemmer on the token text)\n",
        "  stem = stemmer.stem(token.text)\n",
        "  stems.append(stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRSKFmnW6nN3",
        "outputId": "d750139a-69f2-45eb-d7bd-cb811407a209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step-by-Step Processing ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Display Results ---\n",
        "\n",
        "## Tokenization\n",
        "print(\"\\n## Tokenization\")\n",
        "\n",
        "print(tokens)\n",
        "print(\"------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_paexJie7KPW",
        "outputId": "c721ee02-c2a6-4847-f922-3630eb029679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Tokenization\n",
            "['Jack', 'and', 'Jill', 'went', 'up', 'the', 'hill']\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lemmatization (Token + Lemma)\n",
        "print(\"\\n## Lemmatization (Token & Lemma)\")\n",
        "print(f\"{'Token':<7} | {'Lemma':<7}\")\n",
        "print(f\"{'-'*7} | {'-'*7}\")\n",
        "for t, l in zip(tokens, lemmas):\n",
        " print(f\"{t:<7} | {l:<7}\")\n",
        " print(\"------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlZYHSrQ7aFt",
        "outputId": "619c094b-48a0-4836-ebce-5a5597da68a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Lemmatization (Token & Lemma)\n",
            "Token   | Lemma  \n",
            "------- | -------\n",
            "Jack    | Jack   \n",
            "------------------------------\n",
            "and     | and    \n",
            "------------------------------\n",
            "Jill    | Jill   \n",
            "------------------------------\n",
            "went    | go     \n",
            "------------------------------\n",
            "up      | up     \n",
            "------------------------------\n",
            "the     | the    \n",
            "------------------------------\n",
            "hill    | hill   \n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Stop Word Removal (Clean Tokens)\n",
        "print(\"\\n## Stop Word Removal & Final Clean List (Lemmatized)\")\n",
        "print(clean_tokens)\n",
        "print(\"------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0xtGCmt7eAn",
        "outputId": "c9b1333c-ad3e-42bb-e8c7-a104c08d1756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Stop Word Removal & Final Clean List (Lemmatized)\n",
            "['Jack', 'Jill', 'go', 'hill']\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Stemming (Token + Stem)\n",
        "print(\"\\n## Stemming (Token & Stem - using NLTK)\")\n",
        "# Note: Stemming is generally less accurate than lemmatization, e.g., 'running' -> 'run' (lemma) vs 'run' (stem)\n",
        "print(f\"{'Token':<7} | {'Stem':<7}\")\n",
        "print(f\"{'-'*7} | {'-'*7}\")\n",
        "for t, s in zip(tokens, stems):\n",
        "  print(f\"{t:<7} | {s:<7}\")\n",
        "  print(\"------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaVvTbsH7j-m",
        "outputId": "f4bb6301-2dac-413a-b458-a89a3e50aec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Stemming (Token & Stem - using NLTK)\n",
            "Token   | Stem   \n",
            "------- | -------\n",
            "Jack    | jack   \n",
            "------------------------------\n",
            "and     | jill   \n",
            "------------------------------\n",
            "Jill    | went   \n",
            "------------------------------\n",
            "went    | hill   \n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}